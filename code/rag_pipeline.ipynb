{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-23T14:30:02.874345Z",
     "start_time": "2024-03-23T14:29:59.962692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "# Installs\n",
    "%pip install -q langchain langchain-community langchain-openai fastembed qdrant-client oxrdflib os operator typing numpy --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T11:04:28.409753Z",
     "start_time": "2024-03-24T11:04:23.109565Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from operator import itemgetter\n",
    "from typing import Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.schema import format_document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders.base import BaseLoader\n",
    "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import get_buffer_string\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import OpenAI\n",
    "from rdflib import Graph"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_openai_api_key(api_key):\n",
    "    client = openai.OpenAI(api_key=api_key)\n",
    "    try:\n",
    "        client.models.list()\n",
    "    except openai.AuthenticationError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-24T11:07:17.734189Z",
     "start_time": "2024-03-24T11:07:17.727188Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T11:07:18.271310Z",
     "start_time": "2024-03-24T11:07:17.868221Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if file exists\n",
    "OPENAI_KEY_FILE_PATH = \"../data/openai-key.txt\"\n",
    "openai_api_key = \"\"\n",
    "if os.path.exists(OPENAI_KEY_FILE_PATH):\n",
    "    f = open(\"../data/openai-key.txt\", \"r\")\n",
    "    lst = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    openai_api_key = lst[-1]\n",
    "while not check_openai_api_key(openai_api_key):\n",
    "    openai_api_key = getpass.getpass(\"Provide your OpenAI API Key\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T11:07:42.129991Z",
     "start_time": "2024-03-24T11:07:42.107471Z"
    }
   },
   "outputs": [],
   "source": [
    "CLASS_QUERY = \"\"\"\n",
    "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "    PREFIX rdfs:  <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX owl:  <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "    PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "    SELECT ?uri ?pred ?label ?type\n",
    "    WHERE {\n",
    "        ?uri a ?type ;\n",
    "            ?pred ?label .\n",
    "        FILTER (\n",
    "            ?type = owl:Class\n",
    "        )\n",
    "        FILTER (\n",
    "            ?pred = rdfs:label ||\n",
    "            ?pred = skos:prefLabel ||\n",
    "            ?pred = skos:altLabel ||\n",
    "            ?pred = skos:definition ||\n",
    "            ?pred = rdfs:comment ||\n",
    "            ?pred = dcterms:description ||\n",
    "            ?pred = dc:title\n",
    "        )\n",
    "    }\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Extracts class labels\n",
    "\"\"\"\n",
    "\n",
    "PROPERTY_QUERY = \"\"\"\n",
    "    PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "    PREFIX rdfs:  <http://www.w3.org/2000/01/rdf-schema#>\n",
    "    PREFIX owl:  <http://www.w3.org/2002/07/owl#>\n",
    "    PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "    PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "    SELECT ?uri ?pred ?label ?type\n",
    "    WHERE {\n",
    "        ?uri a ?type ;\n",
    "            ?pred ?label .\n",
    "        FILTER (\n",
    "            ?type = owl:DatatypeProperty ||\n",
    "            ?type = owl:ObjectProperty\n",
    "        )\n",
    "        FILTER (\n",
    "            ?pred = rdfs:label ||\n",
    "            ?pred = skos:prefLabel ||\n",
    "            ?pred = skos:altLabel ||\n",
    "            ?pred = skos:definition ||\n",
    "            ?pred = rdfs:comment ||\n",
    "            ?pred = dcterms:description ||\n",
    "            ?pred = dc:title\n",
    "        )\n",
    "    }\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Query to extract property labels\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class OntologyLoader(BaseLoader):\n",
    "    \"\"\"\n",
    "    Load an OWL ontology and extract classes and properties as documents.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ontology_url: str, rdf_language_format: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the OntologyLoader.\n",
    "\n",
    "        Args:\n",
    "            ontology_url (str): URL of the OWL ontology to be loaded.\n",
    "            rdf_language_format (str): Format of the OWL ontology to be loaded.\n",
    "        \"\"\"\n",
    "        self.ontology_url = ontology_url\n",
    "        self.format = rdf_language_format\n",
    "        self.graph = Graph(store=\"Oxigraph\")\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"\n",
    "        Load and return documents (classes and properties) from the OWL ontology.\n",
    "        \"\"\"\n",
    "        if self.format:\n",
    "            self.graph.parse(self.ontology_url, format=self.format)\n",
    "        else:\n",
    "            self.graph.parse(self.ontology_url)\n",
    "\n",
    "        # Extract classes and properties as documents\n",
    "        docs: List[Document] = []\n",
    "        for cls in self.graph.query(CLASS_QUERY):\n",
    "            docs.append(self._create_document(cls))\n",
    "        for prop in self.graph.query(PROPERTY_QUERY):\n",
    "            docs.append(self._create_document(prop))\n",
    "        return docs\n",
    "\n",
    "    def _create_document(self, result_row: Any) -> Document:\n",
    "        \"\"\"\n",
    "        Create a Document object from a query result row.\n",
    "        \"\"\"\n",
    "        label = str(result_row.label)\n",
    "        metadata = {\n",
    "            \"label\": label,\n",
    "            \"uri\": str(result_row.uri),\n",
    "            \"type\": str(result_row.type),\n",
    "            \"predicate\": str(result_row.pred),\n",
    "            \"ontology\": self.ontology_url,\n",
    "        }\n",
    "        return Document(page_content=label, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T11:07:42.162Z",
     "start_time": "2024-03-24T11:07:42.131994Z"
    }
   },
   "outputs": [],
   "source": [
    "def prep_retriever(embed_name=\"BAAI/bge-small-en-v1.5\",\n",
    "                   embed_max_length=512,\n",
    "                   ontology_url=\"../data/health.ttl\",\n",
    "                   ontology_format=\"ttl\",\n",
    "                   split_size=1000,\n",
    "                   split_overlap=200,\n",
    "                   k=45):\n",
    "    flag_embeddings = FastEmbedEmbeddings(model_name=embed_name, max_length=embed_max_length)\n",
    "    loader = OntologyLoader(ontology_url=ontology_url, rdf_language_format=ontology_format)\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Split the documents into chunks if necessary\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=split_size, chunk_overlap=split_overlap)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    vectorstore = Qdrant.from_documents(\n",
    "        splits,\n",
    "        flag_embeddings,\n",
    "        collection_name=\"ontologies\",\n",
    "        location=\":memory:\",\n",
    "    )\n",
    "\n",
    "    # K is the number of source documents retrieved\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "\n",
    "    return retriever\n",
    "\n",
    "\n",
    "def prep_llm(temp=0):\n",
    "    llm = OpenAI(temperature=temp)\n",
    "    return llm\n",
    "\n",
    "\n",
    "def prep_memory():\n",
    "    # Create the memory object that is used to add messages\n",
    "    memory = ConversationBufferMemory(return_messages=True, output_key=\"answer\", input_key=\"question\")\n",
    "\n",
    "    # Add a \"memory\" key to the input object\n",
    "    loaded_memory = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"history\"), )\n",
    "\n",
    "    return memory, loaded_memory\n",
    "\n",
    "\n",
    "def prep_prompts(reform_template, answer_template):\n",
    "    reform_question_prompt = PromptTemplate.from_template(reform_template)\n",
    "    answer_prompt = ChatPromptTemplate.from_template(answer_template)\n",
    "    default_document_prompt = PromptTemplate.from_template(\n",
    "        template=\"Concept label: {page_content} | URI: {uri} | Type: {type} | Predicate: {predicate} | Ontology: {ontology}\")\n",
    "\n",
    "    return reform_question_prompt, answer_prompt, default_document_prompt\n",
    "\n",
    "\n",
    "def _combine_documents(docs, document_prompt, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "def prep_chain(reform_question_prompt, llm, retriever, default_document_prompt, answer_prompt, loaded_memory):\n",
    "    # Reformulate the question using chat history\n",
    "    reformulated_question = {\n",
    "        \"reformulated_question\": {\n",
    "                                     \"question\": lambda x: x[\"question\"],\n",
    "                                     \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "                                 }\n",
    "                                 | reform_question_prompt\n",
    "                                 | llm\n",
    "                                 | StrOutputParser(),\n",
    "    }\n",
    "\n",
    "    # Retrieve the documents using the reformulated question\n",
    "    retrieved_documents = {\n",
    "        \"docs\": itemgetter(\"reformulated_question\") | retriever,\n",
    "        \"question\": lambda x: x[\"reformulated_question\"]\n",
    "    }\n",
    "\n",
    "    # Construct the inputs for the final prompt using retrieved documents\n",
    "    final_inputs = {\n",
    "        \"context\": lambda x: _combine_documents(x[\"docs\"], default_document_prompt),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "\n",
    "    # Generate the answer using the retrieved documents and answer prompt\n",
    "    answer = {\n",
    "        \"answer\": final_inputs | answer_prompt | llm,\n",
    "        \"docs\": itemgetter(\"docs\"),\n",
    "    }\n",
    "\n",
    "    # Put the chain together\n",
    "    final_chain = loaded_memory | reformulated_question | retrieved_documents | answer\n",
    "\n",
    "    return final_chain\n",
    "\n",
    "\n",
    "def stream_chain(final_chain, memory: ConversationBufferMemory, memoryless, inputs: dict[str, str]) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Ask question, stream the answer output, and return the answer with source documents.\n",
    "    \"\"\"\n",
    "    output = {\"answer\": \"\"}\n",
    "\n",
    "    for chunk in final_chain.stream(inputs):\n",
    "        if \"docs\" in chunk:\n",
    "            output[\"docs\"] = [doc.dict() for doc in chunk[\"docs\"]]\n",
    "            for _ in output[\"docs\"]:\n",
    "                continue  \n",
    "                \n",
    "        if \"answer\" in chunk:\n",
    "            output[\"answer\"] += chunk[\"answer\"]\n",
    "\n",
    "    if memoryless == 0:\n",
    "        memory.save_context(inputs, {\"answer\": output[\"answer\"]})\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T11:07:42.178002Z",
     "start_time": "2024-03-24T11:07:42.164002Z"
    }
   },
   "outputs": [],
   "source": [
    "def multi_hop(subject, questions, retriever, llm, reform_template, answer_template, memoryless=0):\n",
    "    \"\"\"\n",
    "    According to a given subject and a question list, performs multi-hop reasoning by asking questions in order.\n",
    "\n",
    "    Returns: Two-column array of questions and their answers.\n",
    "    \"\"\"\n",
    "    # Inject the subject into questions\n",
    "    questions[0] = questions[0].format(concept=subject)\n",
    "\n",
    "    # Check if memorylessness is inputted correctly\n",
    "    if memoryless not in [0, 1]:\n",
    "        print(\"Input memoryless parameter as binary.\")\n",
    "        return\n",
    "\n",
    "    # Create empty memory\n",
    "    memory, loaded_memory = prep_memory()\n",
    "    \n",
    "    reform_question_prompt, answer_prompt, default_document_prompt = prep_prompts(reform_template, answer_template)\n",
    "    \n",
    "    final_chain = prep_chain(reform_question_prompt, \n",
    "                             llm, \n",
    "                             retriever, \n",
    "                             default_document_prompt, \n",
    "                             answer_prompt,\n",
    "                             loaded_memory)\n",
    "    questions_and_answers = []\n",
    "\n",
    "    for prompt in questions:\n",
    "        answer = stream_chain(final_chain=final_chain, \n",
    "                              memory=memory, \n",
    "                              memoryless=memoryless,\n",
    "                              inputs={\"question\": prompt})\n",
    "        questions_and_answers.append([f\"Question: {prompt}\", answer[\"answer\"]])\n",
    "\n",
    "    return np.array(questions_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T11:07:42.194006Z",
     "start_time": "2024-03-24T11:07:42.180003Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prompt to reformulate the question using the chat history\n",
    "reforming_prompt = \"\"\"Given the following chat history and a follow up question,\n",
    "rephrase the follow up question to be a standalone straightforward question, in its original language.\n",
    "Do not answer the question! Just rephrase reusing information from the chat history.\n",
    "Make it short and straight to the point.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\n",
    "\"\"\"\n",
    "\n",
    "# Prompt to ask to answer the reformulated question\n",
    "rewritten_prompt = \"\"\"Briefly answer the question based only on the following context,\n",
    "do not use any information outside this context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "# List of prompts for multi-hop reasoning\n",
    "prompts = [\"Can you define me what a {concept} is?\", \"What is similar to this concept?\",\n",
    "         \"What are their objects' URIs with their labels?\"]\n",
    "\n",
    "# Subject to be enquired about\n",
    "subject_of_interest = \"urethritis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T11:10:54.758352Z",
     "start_time": "2024-03-24T11:07:42.196006Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bd9a64da04f46afb6fd9fa62dae3276"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\stefa\\AppData\\Local\\Temp\\fastembed_cache\\models--qdrant--bge-small-en-v1.5-onnx-q. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "document_retriever = prep_retriever()\n",
    "large_language_model = prep_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T11:11:04.670747Z",
     "start_time": "2024-03-24T11:10:54.759352Z"
    }
   },
   "outputs": [],
   "source": [
    "llm_results = multi_hop(subject=subject_of_interest,\n",
    "                        questions=prompts,\n",
    "                        retriever=document_retriever,\n",
    "                        llm=large_language_model,\n",
    "                        reform_template=reforming_prompt,\n",
    "                        answer_template=rewritten_prompt,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T11:11:04.686749Z",
     "start_time": "2024-03-24T11:11:04.671747Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Question: Can you define me what a urethritis is?'\n",
      " '\\nUrethritis is a type of urethral disease that causes inflammation of the urethra, which is the tube that carries urine from the bladder out of the body. It can be caused by various factors, including infections, trauma, or irritation.']\n",
      "['Question: What is similar to this concept?'\n",
      " '\\nPossible answer: Urethral disease, urethral syndrome, urethral obstruction, Chlamydia trachomatis urethritis, urethral stricture, infective urethral stricture, urethral gland abscess, urogenital abnormality, urinary tract obstruction, urethral calculus, gonococcal urethritis, urethral benign neoplasm, autoimmune disease of urogenital tract, urinary tract infection, urethral false passage, urinary schistosomiasis, pyelonephritis, Trichomonas urethritis, chlamydia, Chlamydia pneumonia, urethral diverticulum, urethral intrinsic sphincter deficiency, Ureaplasma urealyticum urethritis, urolithiasis, urogenital tuberculosis, bacteriuria, nephritis, parametritis, Mycoplasma genitalium urethritis, prolapse of urethra, bladder disease, urethra cancer, acute pyelonephritis, chronic pyelonephritis, acute pyelonephritis without lesion of renal medullary necrosis, metastasis to the urethra, pyelitis, urethra squamous cell carcinoma,']\n",
      "[\"Question: What are their objects' URIs with their labels?\"\n",
      " \"\\nAnswer: The objects' URIs and labels for conditions similar to urethritis are:\\n\\n1. URI: http://purl.obolibrary.org/obo/DOID_1384 | Label: obsolete Chlamydia trachomatis urethritis\\n2. URI: http://purl.obolibrary.org/obo/DOID_732 | Label: urethral disease\\n3. URI: http://purl.obolibrary.org/obo/DOID_13498 | Label: urethral syndrome\\n4. URI: http://purl.obolibrary.org/obo/DOID_12577 | Label: urethral obstruction\\n5. URI: http://purl.obolibrary.org/obo/DOID_1412 | Label: bacteriuria\\n6. URI: http://purl.obolibrary.org/obo/DOID_2038 | Label: obsolete urogenital abnormality\\n7. URI: http://purl.obolibrary.org/obo/DOID_9589 | Label: urethral calculus\\n8. URI: http://purl.obolibrary.org/obo/DOID_9877 | Label: urethral gland abscess\\n9. URI: http\"]\n"
     ]
    }
   ],
   "source": [
    "for i in llm_results:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
